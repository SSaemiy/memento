import whisper
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F

class MementoWhisper:
    def __init__(self, device=torch.device("cuda" if torch.cuda.is_available() else "cpu")):
        self.whisper_model = whisper.load_model("small").to(device) 
        
    def transcribe_audio(self, file_path):
        print(f"[INFO] '{file_path}' íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...")
        result = self.whisper_model.transcribe(file_path)
        content = result["text"].strip()
        print(f"[TEXT] ì¸ì‹ëœ ë¬¸ì¥: {content}")
        return content
    
class SentiAnalysis:
    def __init__(self, device=torch.device("cuda" if torch.cuda.is_available() else "cpu")):
        self.model_name = "nlp04/korean_sentiment_analysis_kcelectra"
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name).to(device)  # â¤ GPUë¡œ ì´ë™
        self.model.eval()
        self.selected_labels = {
            0: 'ê¸°ì¨(í–‰ë³µí•œ)',
            5: 'ì¼ìƒì ì¸',
            7: 'ìŠ¬í””(ìš°ìš¸í•œ)',
            8: 'í˜ë“¦(ì§€ì¹¨)',
            9: 'ì§œì¦ë‚¨'
        }
        
    def analyze_emotion(self, content):

        inputs = self.tokenizer(content, return_tensors="pt", truncation=True, padding=True).to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))  # â¤ ì…ë ¥ë„ GPUë¡œ
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = F.softmax(outputs.logits, dim=1)

        filtered_probs = {i: probs[0][i].item() for i in self.selected_labels}
        pred_label = max(filtered_probs, key=filtered_probs.get)

        print(f"[EMOTION] ì˜ˆì¸¡ ê°ì •: {self.selected_labels[pred_label]} (score: {filtered_probs[pred_label]:.2f})")
        return self.selected_labels[pred_label], filtered_probs[pred_label]
    
# ğŸš€ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    audio_file = "test_5m.mp3"

    # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    whisper_instance = MementoWhisper()
    senti_instance = SentiAnalysis()

    # ì¸ìŠ¤í„´ìŠ¤ ë©”ì„œë“œ í˜¸ì¶œ
    content = whisper_instance.transcribe_audio(audio_file)
    senti_instance.analyze_emotion(content)

