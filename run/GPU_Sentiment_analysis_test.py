import whisper
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F

# ğŸ¯ ê°ì • ë¼ë²¨
selected_labels = {
    0: 'ê¸°ì¨(í–‰ë³µí•œ)',
    5: 'ì¼ìƒì ì¸',
    7: 'ìŠ¬í””(ìš°ìš¸í•œ)',
    8: 'í˜ë“¦(ì§€ì¹¨)',
    9: 'ì§œì¦ë‚¨'
}

# ğŸ”„ Whisper: ìŒì„± â†’ í…ìŠ¤íŠ¸
def transcribe_audio(file_path):
    print(f"[INFO] '{file_path}' íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...")
    whisper_model = whisper.load_model("small").to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))  # â¤ GPUë¡œ ì´ë™
    result = whisper_model.transcribe(file_path)
    text = result["text"].strip()
    print(f"[TEXT] ì¸ì‹ëœ ë¬¸ì¥: {text}")
    return text

# ğŸ¤– ê°ì • ë¶„ì„ í•¨ìˆ˜
def analyze_emotion(text):

    model_name = "nlp04/korean_sentiment_analysis_kcelectra"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))  # â¤ GPUë¡œ ì´ë™
    model.eval()

    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))  # â¤ ì…ë ¥ë„ GPUë¡œ
    with torch.no_grad():
        outputs = model(**inputs)
        probs = F.softmax(outputs.logits, dim=1)

    filtered_probs = {i: probs[0][i].item() for i in selected_labels}
    pred_label = max(filtered_probs, key=filtered_probs.get)

    print(f"[EMOTION] ì˜ˆì¸¡ ê°ì •: {selected_labels[pred_label]} (score: {filtered_probs[pred_label]:.2f})")
    return selected_labels[pred_label], filtered_probs[pred_label]

# ğŸš€ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    audio_file = "test_5m.mp3"
    text = transcribe_audio(audio_file)
    analyze_emotion(text)
